def get_embeddings(text):
    inputs = embedding_tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512)
    with torch.no_grad():
        outputs = embedding_model(**inputs)
    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()

# Create a custom embedding function
class CustomHuggingFaceEmbeddings:
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
    
    def embed_documents(self, texts):
        return [get_embeddings(text) for text in texts]
    
    def embed_query(self, text):
        return get_embeddings(text)

embeddings = CustomHuggingFaceEmbeddings(embedding_model, embedding_tokenizer)

# Use the embeddings
vectordb = Chroma.from_documents(documents=texts, embedding=embeddings, persist_directory="chroma_db")
retriever = vectordb.as_retriever()


.spinner {
  display: flex;
  justify-content: center;
  align-items: center;
  height: 100px;
}

.spinner::after {
  content: "";
  width: 50px;
  height: 50px;
  border: 5px solid #f3f3f3;
  border-top: 5px solid #007bff;
  border-radius: 50%;
  animation: spin 1s linear infinite;
}

@keyframes spin {
  0% { transform: rotate(0deg); }
  100% { transform: rotate(360deg); }
}

.answer {
  background-color: #f8f9fa;
  border: 1px solid #e9ecef;
  border-radius: 4px;
  padding: 20px;
  margin-top: 20px;
  white-space: pre-wrap;
  word-wrap: break-word;
}




